- id: doi.org/10.1038/s41598-025-91430-0
  type: paper
  description: Teach-Former, a novel knowledge distillation (KD) framework that leverages a Transformer backbone to effectively condense the knowledge of multiple teacher models into a single, streamlined student model. Moreover, it excels in the contextual and spatial interpretation of relationships across multimodal images for more accurate and precise segmentation.
  date: '2025-05-07'
  image: https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41598-025-91430-0/MediaObjects/41598_2025_91430_Fig2_HTML.png?as=webp
  buttons:
  - type: github
    link: https://github.com/FarihaHossain/TeachFormer
  - type: source
    text: Manuscript Source
    link: https://www.nature.com/articles/s41598-025-91430-0
  tags:
  - knowledge distillation
  - oncology
  repo: TeachFormer

- id: doi:10.1016/j.csbj.2020.05.017
  image: https://ars.els-cdn.com/content/image/1-s2.0-S2001037020302804-gr1.jpg

- id: doi:10.7554/eLife.32822
  image: https://iiif.elifesciences.org/lax:32822%2Felife-32822-fig8-v3.tif/full/863,/0/default.webp